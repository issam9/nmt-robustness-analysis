{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook, you need to compute head influence (the distance of noisy word representations before and after masking a specific head). You can run the following to do so:\n",
    "\n",
    "`Python ./representations/masked_head_distance.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/P70087445/miniconda3/envs/understand/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import pickle\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_attention(weights, heads, df, lang, is_monolingual):\n",
    "    nlp = spacy.load(\"fr_core_news_sm\" if lang=='fr' else \"en_core_web_sm\")\n",
    "    \n",
    "    num_layers = heads.shape[-1]\n",
    "    start_idx = 1\n",
    "    if is_monolingual:\n",
    "        start_idx = 0\n",
    "                \n",
    "    output = {i:defaultdict(list) for i in range(num_layers)}\n",
    "    \n",
    "    for i, sentence_weights in enumerate(weights):\n",
    "        sentence = df.loc[i, 'line']\n",
    "        sentence_tags = [item.pos_ for item in nlp(sentence)]\n",
    "        for j, layer_weights in enumerate(sentence_weights):\n",
    "            influence_head = heads[i][j]\n",
    "            head_weights = layer_weights[influence_head][start_idx:-1]\n",
    "            for k, word_weight in enumerate(head_weights):\n",
    "                if k==int(df.loc[i, 'index']):\n",
    "                    continue\n",
    "                output[j][sentence_tags[k]].append(word_weight)\n",
    "                \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_top_n_longest_lists(d, n=10):\n",
    "    sorted_dict = dict(sorted(d.items(), key=lambda item: len(item[1]), reverse=True))\n",
    "    top_n_dict = dict(list(sorted_dict.items())[:n])\n",
    "    \n",
    "    return top_n_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_tag_attention(tag_attention):\n",
    "    average_dict = defaultdict(list)\n",
    "    for k, item in tag_attention.items():\n",
    "        item = keep_top_n_longest_lists(item)\n",
    "        for tag, attention in item.items():\n",
    "            average_dict['Attention'].append(sum(attention)/len(attention))\n",
    "            average_dict['Tag'].append(tag)\n",
    "            average_dict['Layer'].append(k+1)\n",
    "    return pd.DataFrame(average_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_or_load_attention_csv(df, output_path, src, model, error_type):\n",
    "    csv_path = f'{output_path}{model}/test.{error_type}.{src}.tag_attention.csv'\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(\"yes\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"es\"\n",
    "errors = ['article', 'nounnum', 'prep']\n",
    "\n",
    "fig_model_names = ['OPUS-MT', 'M2M100', 'MBART', 'NLLB']\n",
    "\n",
    "for src in ['en']:\n",
    "    for lang in ['es']:\n",
    "        if src == 'fr' and lang != 'es':\n",
    "            continue\n",
    "\n",
    "        models = {\n",
    "            f'opus-bi-{src}-{lang}': f'opus-mt-{src}-{lang}',\n",
    "            f'm2m100-{src}-{lang}': 'm2m100_418M', \n",
    "            f'mbart-{src}-{lang}': 'mbart-large-50-many-to-many-mmt',\n",
    "            f'nllb-{src}-{lang}': 'nllb-200-distilled-600M'\n",
    "        }\n",
    "        model_types = ['Base', 'Clean', 'Noisy']\n",
    "\n",
    "        fig, axes = plt.subplots(len(models), len(errors), figsize=(32, 24), sharex=True, sharey=True)\n",
    "        fig.suptitle(f'Average Attention Scores for POS Tags on {src.capitalize()}-{lang.capitalize()}', fontsize=20, y=0.95, fontweight='bold')\n",
    "\n",
    "        for i, (finetuned_model, base_model) in enumerate(models.items()):\n",
    "            for j, error in enumerate(errors):\n",
    "                \n",
    "                base_path = f'../outputs/representations'\n",
    "                noisy_file = f'{base_path}/head_masking/{src}-{lang}/{finetuned_model}-{error}/test.{error}.{src}.to_clean.distance.pkl'\n",
    "                clean_file = f'{base_path}/head_masking/{src}-{lang}/{finetuned_model}-clean-{error}/test.{error}.{src}.to_clean.distance.pkl'\n",
    "                base_file = f'{base_path}/head_masking/{src}-{lang}/{base_model}/test.{error}.{src}.to_clean.distance.pkl'\n",
    "                \n",
    "                \n",
    "                noisy_output = load_pickle(noisy_file)\n",
    "                clean_output = load_pickle(clean_file)\n",
    "                base_output = load_pickle(base_file)\n",
    "\n",
    "                noisy_weights_file = f'{base_path}/attention_weights/{src}-{lang}/{finetuned_model}-{error}/test.{error}.{src}.attention_weights.pkl'\n",
    "                clean_weights_file = f'{base_path}/attention_weights/{src}-{lang}/{finetuned_model}-clean-{error}/test.{error}.{src}.attention_weights.pkl'\n",
    "                base_weights_file = f'{base_path}/attention_weights/{src}-{lang}/{base_model}/test.{error}.{src}.attention_weights.pkl'\n",
    "                \n",
    "                noisy_weights = load_pickle(noisy_weights_file)\n",
    "                clean_weights = load_pickle(clean_weights_file)\n",
    "                base_weights = load_pickle(base_weights_file)\n",
    "                \n",
    "                df_file = f'../data/grammar-noise/{src}-{lang}/test.{error}.{src}.pkl'\n",
    "                df = pd.read_pickle(df_file)\n",
    "                df = df[~df['label'].isin(['clean'])].reset_index(drop=True)\n",
    "\n",
    "                noisy_influence_heads = noisy_output.argmax(-1)\n",
    "                clean_influence_heads = clean_output.argmax(-1)\n",
    "                base_influence_heads = base_output.argmax(-1)\n",
    "\n",
    "                is_monolingual = 'opus' in finetuned_model\n",
    "                \n",
    "                noisy_tag_attention = get_tag_attention(noisy_weights, noisy_influence_heads, df, src, is_monolingual)\n",
    "                clean_tag_attention = get_tag_attention(clean_weights, clean_influence_heads, df, src, is_monolingual)\n",
    "                base_tag_attention = get_tag_attention(base_weights, base_influence_heads, df, src, is_monolingual)\n",
    "                \n",
    "                noisy_df = get_average_tag_attention(noisy_tag_attention)\n",
    "                clean_df = get_average_tag_attention(clean_tag_attention)\n",
    "                base_df = get_average_tag_attention(base_tag_attention)\n",
    "                \n",
    "                output_path = f'../outputs/representations/attention_weights/{src}-{lang}/'\n",
    "                \n",
    "                noisy_df = save_or_load_attention_csv(noisy_df, output_path, src, f'{finetuned_model}-{error}', error)\n",
    "                clean_df = save_or_load_attention_csv(clean_df, output_path, src, f'{finetuned_model}-clean-{error}', error)\n",
    "                base_df = save_or_load_attention_csv(base_df, output_path, src, base_model, error)\n",
    "                \n",
    "                dfs = [base_df, clean_df, noisy_df]\n",
    "                vmin = min(df['Attention'].min() for df in dfs)\n",
    "                vmax = max(df['Attention'].max() for df in dfs)\n",
    "\n",
    "                for k, (df, title) in enumerate(zip(dfs, model_types)):\n",
    "                    pos = axes[i, j].get_position()\n",
    "                    width = pos.width / 3.3\n",
    "                    new_pos = [pos.x0 + k * (width * 1.1), pos.y0, width, pos.height]\n",
    "                    sub_ax = fig.add_axes(new_pos)\n",
    "                    \n",
    "                    show_y_labels = (k == 0)\n",
    "                    show_x_labels = (i == len(models) - 1)\n",
    "\n",
    "                    df['Layer'] = pd.Categorical(df['Layer'], categories=range(1, int(df['Layer'].max())+1), ordered=True)\n",
    "                    pivot_data = df.pivot(index=\"Layer\", columns=\"Tag\", values=\"Attention\")\n",
    "                    \n",
    "                    sns.heatmap(pivot_data, ax=sub_ax, cmap=\"YlGnBu\", vmin=vmin, vmax=vmax, cbar=False, annot=False, fmt='.2f')\n",
    "                    \n",
    "                    sub_ax.set_title(title, fontsize=10, fontweight='bold')\n",
    "\n",
    "                    if show_y_labels:\n",
    "                        sub_ax.set_ylabel('Layer', fontsize=10, fontweight='bold')\n",
    "                        sub_ax.set_yticklabels(range(1, int(df['Layer'].max())+1), rotation=90, ha='right', fontsize=8, fontweight='bold')\n",
    "                    else:\n",
    "                        sub_ax.set_ylabel('')\n",
    "                        sub_ax.set_yticklabels([])\n",
    "\n",
    "                    if show_x_labels and k == 1:\n",
    "                        sub_ax.set_xlabel('Tag', fontsize=10, fontweight='bold')\n",
    "                    else:\n",
    "                        sub_ax.set_xlabel('')\n",
    "                    sub_ax.set_xticklabels(pivot_data.columns, rotation=90, ha='right', fontsize=8, fontweight='bold')\n",
    "\n",
    "                axes[i, j].remove()\n",
    "\n",
    "        for ax, col in zip(axes[0], errors):\n",
    "            fig.text(ax.get_position().x0 + ax.get_position().width / 2, 0.91, col.capitalize(), ha='center', va='bottom', fontsize=16, fontweight='bold')\n",
    "\n",
    "        for ax, row in zip(axes[:, 0], fig_model_names):\n",
    "            fig.text(0.09, ax.get_position().y0 + ax.get_position().height / 2, row, ha='right', va='center', fontsize=16, fontweight='bold', rotation=90)\n",
    "\n",
    "        plt.subplots_adjust(hspace=0., wspace=0.1)\n",
    "        plt.savefig(f'../figures/attention_to_pos/attention_{src}-{lang}.pdf', dpi=300, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
